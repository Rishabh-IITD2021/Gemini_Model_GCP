{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bUatXNtBJ0hW",
        "outputId": "6285a54e-3f43-4921-cd80-c77de2907b89"
      },
      "id": "bUatXNtBJ0hW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.7.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import pandas as pd\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting,GenerationConfig"
      ],
      "metadata": {
        "id": "k4FCpUCCrbm-"
      },
      "id": "k4FCpUCCrbm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "2wAYxjIdgQ2lsv1PDf0CG2Sq",
      "metadata": {
        "tags": [],
        "id": "2wAYxjIdgQ2lsv1PDf0CG2Sq"
      },
      "source": [
        "def generate(Prompt,system_prompt,generation_config,safety_settings ):\n",
        "    vertexai.init(project=\"hclsw-gcp-srsch\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\n",
        "        \"gemini-1.5-flash-001\",\n",
        "        system_instruction=[system_prompt]\n",
        "    )\n",
        "    responses = model.generate_content(\n",
        "        [Prompt],\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings,\n",
        "        stream=False,\n",
        "    )\n",
        "    return responses\n",
        "    # for response in responses:\n",
        "    #     print(response.text, end=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_template_6(Jira_story):\n",
        "    Prompt = f\"\"\"\n",
        "    Please break down the following Jira Story into appropriate Subtasks based on its context and the goals outlined in the story. Each subtask should have a clear title and description, outlining the key actions and objectives. Ensure that the subtasks align with the parent story's context and overall goals.\n",
        "\n",
        "    - **User Input:**\n",
        "      - Jira Story Details: Includes Title, Summary, Context, and Acceptance Criteria.\n",
        "\n",
        "    - **Expected Output:**\n",
        "      - **Subtasks**: A structured breakdown of tasks with clear descriptions. Each subtask should focus on a specific action related to the parent story.\n",
        "\n",
        "    - **Example Format:**\n",
        "\n",
        "      **User Input:**\n",
        "      - **Parent Story:**\n",
        "\n",
        "        **Story 1: Generate Reports for Multiple Use Cases**\n",
        "\n",
        "        - **Summary**: The system should be able to generate reports for various use cases.\n",
        "\n",
        "        - **Context**: Users need to understand the results of the model across different scenarios. Reports provide a structured way to present this information.\n",
        "\n",
        "        - **Acceptance Criteria**:\n",
        "          - The system should support the generation of reports for at least three distinct use cases.\n",
        "          - Each report should contain relevant information specific to the use case, such as data used, model parameters, and key findings.\n",
        "          - Users should be able to select the use cases for which they want to generate reports.\n",
        "\n",
        "      **Output:**\n",
        "\n",
        "      - **Story: Generate Reports for Multiple Use Cases**\n",
        "\n",
        "        - **Subtask 1: Identify Data and Parameters for Each Report**\n",
        "\n",
        "          - **Description**: Identify the specific data and parameters needed for generating the reports for each use case.\n",
        "\n",
        "        - **Subtask 2: Design Report Templates**\n",
        "\n",
        "          - **Description**: Design templates for the reports that will display the relevant information.\n",
        "\n",
        "        - **Subtask 3: Implement Report Generation Logic**\n",
        "\n",
        "          - **Description**: Develop the logic needed to generate reports based on user selection.\n",
        "\n",
        "        - **Subtask 4: Enable User Selection of Use Cases**\n",
        "\n",
        "          - **Description**: Implement a user interface that allows users to select the use cases for which they want to generate reports.\n",
        "\n",
        "    **Important Notes:**\n",
        "    - Do **not** assign Story Points to the subtasks.\n",
        "    - Do **not** analyze or recommend changes to the parent story’s Story Points.\n",
        "    - Focus on breaking down the story into clear, actionable tasks.\n",
        "    - Each subtask should be concise and aligned with the overall goal of the parent story.\n",
        "\n",
        "    Now, based on the following Jira Story, generate the appropriate subtasks:\n",
        "\n",
        "    **Parent Story: {Jira_story}**\n",
        "    \"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You will receive a Jira Story containing details such as Context, Acceptance Criteria, and Summary.\n",
        "\n",
        "    Your task is to:\n",
        "\n",
        "    - **Analyze the Story**: Understand the context, goals, and objectives of the story.\n",
        "    - **Break Down into Subtasks**: Decompose the story into smaller, actionable tasks based on the given context.\n",
        "    - **Provide Clear Subtask Descriptions**: Write each subtask description in a way that is easy to understand and actionable.\n",
        "\n",
        "    **Output Requirements:**\n",
        "    - Each subtask should have a **Title** and a **Description**.\n",
        "    - Do **not** assign Story Points to the subtasks.\n",
        "    - Do **not** analyze or suggest changes to the parent story’s Story Points.\n",
        "\n",
        "    **Guidelines:**\n",
        "    - Each subtask should focus on one specific action or goal within the context of the parent story.\n",
        "    - Ensure that the subtasks are concise and actionable.\n",
        "    - The subtasks must align with the overall objective and context of the parent story.\n",
        "    \"\"\"\n",
        "\n",
        "    return Prompt, system_prompt\n"
      ],
      "metadata": {
        "id": "whnSvswYev4c"
      },
      "id": "whnSvswYev4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(Jira_story):\n",
        "\n",
        "  prompt , system_prompt = prompt_template_6(Jira_story)\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 1024,\n",
        "      \"temperature\": 0.5,\n",
        "      \"top_p\": 0.95,\n",
        "      \"seed\":42\n",
        "  }\n",
        "\n",
        "  safety_settings = [\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "    ]\n",
        "\n",
        "  result = generate(prompt,system_prompt,generation_config,safety_settings)\n",
        "\n",
        "  return result.text\n",
        "\n"
      ],
      "metadata": {
        "id": "yhrD_srHZxNg"
      },
      "id": "yhrD_srHZxNg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_2(Jira_story):\n",
        "\n",
        "  prompt , system_prompt = prompt_template_6(Jira_story)\n",
        "\n",
        "  generation_config = {\n",
        "    \"max_output_tokens\": 1024,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\": 0.95,\n",
        "    \"response_mime_type\": \"application/json\",\n",
        "    \"response_schema\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"stories\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"story\": {\"type\": \"STRING\"},\n",
        "                        \"subtasks\": {\n",
        "                            \"type\": \"ARRAY\",\n",
        "                            \"items\": {\n",
        "                                \"type\": \"OBJECT\",\n",
        "                                \"properties\": {\n",
        "                                    \"subtask\": {\"type\": \"STRING\"},\n",
        "                                    \"description\": {\"type\": \"STRING\"}\n",
        "                                }\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"seed\": 42,\n",
        "  }\n",
        "\n",
        "\n",
        "  safety_settings = [\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "      SafetySetting(\n",
        "          category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "          threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "      ),\n",
        "    ]\n",
        "\n",
        "  result = generate(prompt,system_prompt,generation_config,safety_settings)\n",
        "\n",
        "  return result.text\n",
        "\n"
      ],
      "metadata": {
        "id": "sYN5cqypfV9f"
      },
      "id": "sYN5cqypfV9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to format stories for inference\n",
        "def format_stories(stories):\n",
        "    formatted_list = []\n",
        "    for story in stories:\n",
        "        formatted_story = f\"{story['story_number']}: {story['title']}\\n\\n\"\n",
        "        formatted_story += f\"    - Summary: {story['summary']}\\n\\n\"\n",
        "        formatted_story += f\"    - Context: {story['context']}\\n\\n\"\n",
        "        formatted_story += f\"    - Acceptance Criteria:\\n\"\n",
        "\n",
        "        for criteria in story[\"acceptance_criteria\"]:\n",
        "            formatted_story += f\"      - {criteria}\\n\"\n",
        "\n",
        "        formatted_story += f\"\\n    - Story Points: {story['story_points']}\\n\\n\"\n",
        "\n",
        "        formatted_list.append(formatted_story)\n",
        "\n",
        "    return formatted_list\n",
        "\n",
        "# Function to process uploaded JSON file\n",
        "def process_json_file(file, start_idx, end_idx):\n",
        "    df = pd.read_json(file.name)\n",
        "\n",
        "    start_idx = max(0, min(start_idx, len(df) - 1))\n",
        "    end_idx = max(start_idx, min(end_idx, len(df)))\n",
        "\n",
        "    df = df.iloc[start_idx:end_idx]\n",
        "\n",
        "    output_data = []  # Final output list to store processed results\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_data = row.to_dict()  # Keep all original fields\n",
        "        processed_data = []  # Reset processed data for each row\n",
        "\n",
        "        try:\n",
        "            data = json.loads(row['generated_stories'])  # Parse JSON from 'Answer' column\n",
        "            formatted_stories = format_stories(data[\"stories\"])  # Format each story\n",
        "\n",
        "            # Apply inference_2 on each formatted story and store results\n",
        "            for story in formatted_stories:\n",
        "                generated_response = json.loads(inference_2(story))\n",
        "                processed_data.append(generated_response)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        # Map processed data back to the original row\n",
        "        row_data[\"Processed_Answers\"] = processed_data\n",
        "        output_data.append(row_data)\n",
        "\n",
        "    # Save output to JSON file\n",
        "    output_file = \"processed_output.json\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(output_data, f, indent=4)\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# Function to process direct story input using inference\n",
        "def process_direct_input(story):\n",
        "    generated_response = inference(story)  # Apply inference directly\n",
        "\n",
        "    return generated_response  # Display output directly in UI\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WlUuAj-RpDx4"
      },
      "id": "WlUuAj-RpDx4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "def gradio_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## JSON Story Processing To Subtask\")\n",
        "\n",
        "        with gr.Tab(\"Upload and Process JSON\"):\n",
        "            file_input = gr.File(label=\"Upload JSON File\")\n",
        "            start_idx = gr.Number(label=\"Start Index\", value=0, interactive=True)\n",
        "            end_idx = gr.Number(label=\"End Index\", value=10, interactive=True)\n",
        "            file_output = gr.File(label=\"Processed JSON Output\", interactive=False)\n",
        "            process_button = gr.Button(\"Process JSON\")\n",
        "\n",
        "            process_button.click(process_json_file, inputs=[file_input, start_idx, end_idx], outputs=file_output)\n",
        "\n",
        "        with gr.Tab(\"Direct Story Processing\"):\n",
        "            story_input = gr.Textbox(label=\"Enter Story\", lines=5, placeholder=\"Paste your story here\")\n",
        "            response_output = gr.Markdown(label=\"Generated Response\")\n",
        "            generate_button = gr.Button(\"Generate Response\")\n",
        "\n",
        "            generate_button.click(process_direct_input, inputs=[story_input], outputs=response_output)\n",
        "\n",
        "    return demo"
      ],
      "metadata": {
        "id": "JCtFCVTfqVUh"
      },
      "id": "JCtFCVTfqVUh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio interface\n",
        "gradio_interface().launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "4aCX6BP9pNvS",
        "outputId": "2fd702e7-cc3e-4b38-b4f0-feb3c96df134"
      },
      "id": "4aCX6BP9pNvS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://650bcd42db9612a4fb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://650bcd42db9612a4fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## experiments"
      ],
      "metadata": {
        "id": "mRSMoyd0qBU8"
      },
      "id": "mRSMoyd0qBU8"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"/content/processed_output.json\")"
      ],
      "metadata": {
        "id": "m-I9vVytKjZs"
      },
      "id": "m-I9vVytKjZs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON\n",
        "data = json.loads(df['Answer'][0])\n",
        "\n",
        "# Function to format stories and store in a list\n",
        "def format_stories(stories):\n",
        "    formatted_list = []\n",
        "    for story in stories:\n",
        "        formatted_story = f\"{story['story_number']}: {story['title']}\\n\\n\"\n",
        "        formatted_story += f\"    - Summary: {story['summary']}\\n\\n\"\n",
        "        formatted_story += f\"    - Context: {story['context']}\\n\\n\"\n",
        "        formatted_story += f\"    - Acceptance Criteria:\\n\"\n",
        "\n",
        "        for criteria in story[\"acceptance_criteria\"]:\n",
        "            formatted_story += f\"      - {criteria}\\n\"\n",
        "\n",
        "        formatted_story += f\"\\n    - Story Points: {story['story_points']}\\n\\n\"\n",
        "\n",
        "        formatted_list.append(formatted_story)\n",
        "\n",
        "    return formatted_list\n",
        "\n",
        "# Store formatted stories in a list\n",
        "formatted_stories_list = format_stories(data[\"stories\"])\n",
        "\n",
        "# Print each formatted story\n",
        "for story in formatted_stories_list:\n",
        "    print(story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R4z5Wn8maro3",
        "outputId": "99a87b74-57ed-4131-e03e-a66acaa5414c"
      },
      "id": "R4z5Wn8maro3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story 1: Prompt Handling Interface\n",
            "\n",
            "    - Summary: The system should provide a user interface for asking analysis questions.\n",
            "\n",
            "    - Context: Users should be able to interact with the system through a dedicated interface to ask their analysis questions.\n",
            "\n",
            "    - Acceptance Criteria:\n",
            "      - The system should provide a user interface for entering analysis-related questions.\n",
            "      - The interface should be intuitive and user-friendly.\n",
            "      - The system should be able to handle various question formats and structures.\n",
            "      - The system should be able to interpret and understand the user's intent from the question.\n",
            "      - The system should be able to provide feedback on the quality of the question, such as clarity and completeness.\n",
            "      - The system should be able to suggest alternative ways to phrase the question for better results.\n",
            "\n",
            "    - Story Points: 5\n",
            "\n",
            "\n",
            "Story 2: Prompt Optimization Recommendations\n",
            "\n",
            "    - Summary: The system should provide recommendations on how to create optimal prompts.\n",
            "\n",
            "    - Context: The system should help users create effective prompts by offering guidance and suggestions.\n",
            "\n",
            "    - Acceptance Criteria:\n",
            "      - The system should analyze the user's prompt and identify potential areas for improvement.\n",
            "      - The system should provide recommendations on how to optimize the prompt for better results.\n",
            "      - Recommendations should be tailored to the specific question and the context of the analysis.\n",
            "      - Recommendations should be clear, concise, and actionable.\n",
            "      - The system should provide explanations for the recommendations to help users understand the rationale behind them.\n",
            "      - The system should allow users to experiment with different prompt variations to see the impact on the results.\n",
            "\n",
            "    - Story Points: 8\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_stories_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iiFdLGDa0UK",
        "outputId": "dad79429-632b-4900-e4d7-e466a97ffa4a"
      },
      "id": "0iiFdLGDa0UK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story 1: Prompt Handling Interface\n",
            "\n",
            "    - Summary: The system should provide a user interface for asking analysis questions.\n",
            "\n",
            "    - Context: Users should be able to interact with the system through a dedicated interface to ask their analysis questions.\n",
            "\n",
            "    - Acceptance Criteria:\n",
            "      - The system should provide a user interface for entering analysis-related questions.\n",
            "      - The interface should be intuitive and user-friendly.\n",
            "      - The system should be able to handle various question formats and structures.\n",
            "      - The system should be able to interpret and understand the user's intent from the question.\n",
            "      - The system should be able to provide feedback on the quality of the question, such as clarity and completeness.\n",
            "      - The system should be able to suggest alternative ways to phrase the question for better results.\n",
            "\n",
            "    - Story Points: 5\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = inference_2(formatted_stories_list[1])"
      ],
      "metadata": {
        "id": "5SLoTuCtbBc1"
      },
      "id": "5SLoTuCtbBc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj1oc4XafvvD",
        "outputId": "717e7c12-fd62-44ee-820c-807b0519cd95"
      },
      "id": "pj1oc4XafvvD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"stories\": [{\"story\": \"Story 2: Prompt Optimization Recommendations\", \"subtasks\": [{\"description\": \"Analyze user prompts to identify areas for improvement based on factors like clarity, specificity, and potential biases.\", \"subtask\": \"Analyze User Prompts\"}, {\"description\": \"Develop algorithms to generate recommendations for prompt optimization, including suggestions for rephrasing, adding context, specifying desired output format, and incorporating relevant keywords.\", \"subtask\": \"Develop Optimization Algorithms\"}, {\"description\": \"Design an interface to present recommendations to users in a clear and actionable manner, highlighting potential issues and providing specific suggestions for improvement.\", \"subtask\": \"Design Recommendation Interface\"}, {\"description\": \"Implement a feature that allows users to experiment with different prompt variations based on the provided recommendations and observe the impact on the results.\", \"subtask\": \"Implement Prompt Variation Experimentation\"}]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a77kD4MHe1HT",
        "outputId": "78434cc0-910a-4cbf-de66-6c01239ee293"
      },
      "id": "a77kD4MHe1HT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Story: Prompt Optimization Recommendations\n",
            "\n",
            "### Subtasks:\n",
            "\n",
            "**Subtask 1: Analyze Prompt Structure and Content**\n",
            "\n",
            "- **Description:** Develop algorithms to analyze user-provided prompts, identifying key components like keywords, context, and structure. This analysis should identify potential areas for improvement based on best practices and common prompt optimization techniques.\n",
            "\n",
            "**Subtask 2: Develop Recommendation Engine**\n",
            "\n",
            "- **Description:** Implement a recommendation engine that analyzes the prompt analysis results and generates tailored suggestions for optimization. These recommendations should include specific changes to prompt wording, structure, or content, aimed at improving clarity, relevance, and effectiveness.\n",
            "\n",
            "**Subtask 3: Generate Actionable Recommendations**\n",
            "\n",
            "- **Description:** Ensure that the recommendations generated by the system are clear, concise, and actionable. They should be presented in a user-friendly format, providing specific instructions on how to modify the prompt for better results.\n",
            "\n",
            "**Subtask 4: Provide Explanation for Recommendations**\n",
            "\n",
            "- **Description:** Implement a mechanism to provide explanations for the recommendations. This could include highlighting specific parts of the prompt and explaining how the suggested changes will impact the results. This will help users understand the rationale behind the recommendations and make informed decisions.\n",
            "\n",
            "**Subtask 5: Implement Prompt Variation Testing**\n",
            "\n",
            "- **Description:** Develop a feature that allows users to experiment with different prompt variations based on the provided recommendations. This feature should enable users to see the impact of the suggested changes on the results and validate the effectiveness of the recommendations. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0QItex7GeGS9",
        "outputId": "3709bfd5-f831-4739-c4a9-5800c66dd8b4"
      },
      "id": "0QItex7GeGS9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Story 2: Prompt Optimization Recommendations\n",
            "\n",
            "### Subtasks:\n",
            "\n",
            "**Subtask 1: Analyze User Prompts**\n",
            "\n",
            "- Description: Develop a system that can analyze user-provided prompts to identify potential areas for improvement. This includes understanding the prompt's structure, identifying keywords, and analyzing the context of the question.\n",
            "\n",
            "**Subtask 2: Define Optimization Criteria**\n",
            "\n",
            "- Description: Define the criteria for evaluating prompt quality. This could include factors like clarity, relevance, specificity, and potential for bias.\n",
            "\n",
            "**Subtask 3: Develop Recommendation Engine**\n",
            "\n",
            "- Description: Develop an algorithm that can generate recommendations based on the analysis of the user's prompt and the defined optimization criteria. Recommendations should be tailored to the specific question and context.\n",
            "\n",
            "**Subtask 4: Implement Recommendation Display**\n",
            "\n",
            "- Description: Design and implement a user interface that presents the recommendations in a clear and concise manner. The interface should include the recommended changes, explanations for the rationale behind the recommendations, and an option to experiment with different prompt variations.\n",
            "\n",
            "**Subtask 5: Integrate with Existing System**\n",
            "\n",
            "- Description: Integrate the prompt optimization functionality into the existing system, ensuring seamless interaction with other components and user workflows.\n",
            "\n",
            "### Story Points Analysis:\n",
            "\n",
            "After analyzing the subtasks, the effort required for this story appears to be **higher than the initially allocated 8 Story Points**. This is due to the complexity of the recommendation engine, the need for a robust analysis system, and the user interface requirements.\n",
            "\n",
            "**Recommendation**: Increase the Story Points for the parent story to **13** to better reflect the expected effort and ensure the smooth execution of all subtasks. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Vh0F87ZAcIsP",
        "outputId": "dfdeddd5-2ede-49af-d7a5-d2492d61df49"
      },
      "id": "Vh0F87ZAcIsP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Story: Prompt Handling Interface\n",
            "\n",
            "### Subtasks:\n",
            "\n",
            "**Subtask 1: Design User Interface for Question Input**\n",
            "\n",
            "- Description: Design the user interface for entering analysis questions, ensuring it is intuitive and user-friendly. This includes elements like text boxes, buttons, and potentially a visual editor for formatting.\n",
            "\n",
            "**Subtask 2: Implement Question Input Handling**\n",
            "\n",
            "- Description: Implement the backend logic to receive and process user-entered questions. This includes handling different question formats, structures, and potential variations in phrasing.\n",
            "\n",
            "**Subtask 3: Develop Question Interpretation Logic**\n",
            "\n",
            "- Description: Develop the logic to interpret and understand the user's intent from the question. This might involve natural language processing (NLP) techniques to extract key concepts, entities, and relationships from the question.\n",
            "\n",
            "**Subtask 4: Implement Question Quality Feedback**\n",
            "\n",
            "- Description: Implement a mechanism to provide feedback to the user on the quality of their question, considering factors like clarity, completeness, and potential ambiguity.\n",
            "\n",
            "**Subtask 5: Implement Question Suggestion Feature**\n",
            "\n",
            "- Description: Implement a feature to suggest alternative ways to phrase the question for better results. This could involve using NLP techniques to identify potential improvements in wording or structure.\n",
            "\n",
            "### Story Points Analysis\n",
            "\n",
            "Based on the breakdown of the story into these five subtasks, the total effort required seems to be higher than the originally allocated Story Points of 5. The tasks involving question interpretation, quality feedback, and suggestions require significant effort in terms of NLP techniques and algorithm development.\n",
            "\n",
            "**Recommendation:** Increase the Story Points for the parent story to **8** to better reflect the expected effort and ensure the smooth execution of all subtasks. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Answer'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "WBGRyD40aE3f",
        "outputId": "1553ca44-b943-425f-9d02-22f647a9b7e2"
      },
      "id": "WBGRyD40aE3f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"stories\": [{\"acceptance_criteria\": [\"The system should provide a user interface for entering analysis-related questions.\", \"The interface should be intuitive and user-friendly.\", \"The system should be able to handle various question formats and structures.\", \"The system should be able to interpret and understand the user\\'s intent from the question.\", \"The system should be able to provide feedback on the quality of the question, such as clarity and completeness.\", \"The system should be able to suggest alternative ways to phrase the question for better results.\"], \"context\": \"Users should be able to interact with the system through a dedicated interface to ask their analysis questions.\", \"story_number\": \"Story 1\", \"story_points\": \"5\", \"summary\": \"The system should provide a user interface for asking analysis questions.\", \"title\": \"Prompt Handling Interface\"}, {\"acceptance_criteria\": [\"The system should analyze the user\\'s prompt and identify potential areas for improvement.\", \"The system should provide recommendations on how to optimize the prompt for better results.\", \"Recommendations should be tailored to the specific question and the context of the analysis.\", \"Recommendations should be clear, concise, and actionable.\", \"The system should provide explanations for the recommendations to help users understand the rationale behind them.\", \"The system should allow users to experiment with different prompt variations to see the impact on the results.\"], \"context\": \"The system should help users create effective prompts by offering guidance and suggestions.\", \"story_number\": \"Story 2\", \"story_points\": \"8\", \"summary\": \"The system should provide recommendations on how to create optimal prompts.\", \"title\": \"Prompt Optimization Recommendations\"}]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = json.loads(df['Answer'][0])"
      ],
      "metadata": {
        "id": "ucmijzmcLFQP"
      },
      "id": "ucmijzmcLFQP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['stories'][0]['story_number']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wlt2KgnPLGgs",
        "outputId": "5d8028e3-3f7b-4f0f-f7bc-c875b35fb4a7"
      },
      "id": "wlt2KgnPLGgs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Story 1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_template_5(Jira_story):\n",
        "    Prompt = f\"\"\"\n",
        "    Please break down the following Jira Story into appropriate Subtasks based on its context and the goals outlined in the story. Each subtask should have a clear description, outlining the key actions and objectives, based on the context of the story. Additionally, after breaking down the subtasks, please analyze if the total effort required seems to exceed the expected Story Points for the parent story. If the total effort feels higher than expected, provide a **recommendation** to adjust the Story Points for the parent story.\n",
        "\n",
        "    - User Input:\n",
        "       - Jira Story Details: Includes Title, Summary, Context, and Acceptance Criteria. Story Points for the parent story.\n",
        "\n",
        "    - Expected Output:\n",
        "\n",
        "      - Subtasks: A logical breakdown of tasks with clear descriptions. Each subtask should be aligned with the context of the parent story, ensuring that the subtasks follow the overall goals.\n",
        "\n",
        "      - **Story Points Analysis**: After analyzing the parent story and its subtasks, evaluate if the total effort required seems higher than expected. If the total effort required exceeds the parent story's points, **recommend** an increase in the Story Points and suggest an appropriate Fibonacci number for the new value.\n",
        "\n",
        "      - **Example Format**:\n",
        "\n",
        "        User Input:\n",
        "\n",
        "        - Parent Story:\n",
        "\n",
        "          Story 1: Generate Reports for Multiple Use Cases\n",
        "\n",
        "            - Summary: The system should be able to generate reports for various use cases.\n",
        "\n",
        "            - Context: Users need to understand the results of the model across different scenarios. Reports provide a structured way to present this information.\n",
        "\n",
        "            - Acceptance Criteria:\n",
        "\n",
        "              - The system should support the generation of reports for at least three distinct use cases.\n",
        "\n",
        "              - Each report should contain relevant information specific to the use case, such as data used, model parameters, and key findings.\n",
        "\n",
        "              - Users should be able to select the use cases for which they want to generate reports.\n",
        "\n",
        "            - Story Points: 5\n",
        "\n",
        "        Output:\n",
        "\n",
        "          - Story: Generate Reports for Multiple Use Cases\n",
        "\n",
        "            - Subtask 1: Identify Data and Parameters for Each Report\n",
        "\n",
        "              - Description: Identify the specific data and parameters needed for generating the reports for each use case.\n",
        "\n",
        "            - Subtask 2: Design Report Templates\n",
        "\n",
        "              - Description: Design templates for the reports that will display the relevant information.\n",
        "\n",
        "            - Subtask 3: Implement Report Generation Logic\n",
        "\n",
        "              - Description: Develop the logic needed to generate reports based on user selection.\n",
        "\n",
        "            - Subtask 4: Enable User Selection of Use Cases\n",
        "\n",
        "              - Description: Implement a user interface that allows users to select the use cases for which they want to generate reports.\n",
        "\n",
        "            - Story Points Analysis: After analyzing the total effort required for the subtasks, it appears the work may exceed the originally allocated Story Points of 5. **Recommendation**: Increase the Story Points for the parent story to **8** to better reflect the expected effort and ensure the smooth execution of all subtasks.\n",
        "\n",
        "      **Important Note:**\n",
        "      - Do not assign Story Points to the subtasks in your response.\n",
        "      - Focus on breaking down the story into clear, actionable tasks.\n",
        "      - Provide a **recommendation** for adjusting the parent story's points only if you feel the total effort required exceeds the expected Story Points.\n",
        "\n",
        "      Now, based on the context provided in the following Jira Story, generate the appropriate subtasks:\n",
        "\n",
        "      Parent Story: {Jira_story}\n",
        "    \"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You will receive a Jira Story containing details such as Context, Acceptance Criteria, and Summary, along with the allocated Story Points for the parent story.\n",
        "\n",
        "    Your task is to:\n",
        "\n",
        "  - Analyze the Story: Fully understand the context, goals, and objectives of the story.\n",
        "  - Break Down into Subtasks: Decompose the story into smaller, actionable tasks based on the given context. Ensure each subtask is aligned with the overall goals of the parent story.\n",
        "  - Analyze Story Points: Evaluate if the total effort required for the subtasks seems higher than the expected Story Points for the parent story.\n",
        "    - If the total effort for all subtasks exceeds the parent story’s points, **recommend** an increase in the parent story's Story Points, suggesting the appropriate Fibonacci number (e.g., 8).\n",
        "  - Provide Clear Descriptions: Write each subtask description in a way that is easy to understand and actionable, considering the context provided.\n",
        "\n",
        "  Output Requirements:\n",
        "  For each subtask:\n",
        "\n",
        "  - Provide a Title for the task.\n",
        "  - Write a Description that explains the task's purpose and what needs to be done.\n",
        "  - **Do not assign Story Points** for the subtasks in your response.\n",
        "  - Only recommend a change to the parent story’s Story Points if the total effort for the subtasks seems to exceed the points allocated.\n",
        "\n",
        "  Guidelines:\n",
        "  - Each subtask should be focused on one specific action or goal within the context of the parent story.\n",
        "  - Ensure that the subtasks are concise and actionable, making them clear for the team.\n",
        "  - The subtasks must align with the overall objective and context of the parent story.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    return Prompt, system_prompt\n"
      ],
      "metadata": {
        "id": "YPCdW2CkKLYO"
      },
      "id": "YPCdW2CkKLYO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2D9j4w-sL0P0"
      },
      "id": "2D9j4w-sL0P0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "json_response_subtask.ipynb",
      "collapsed_sections": [
        "mRSMoyd0qBU8"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}